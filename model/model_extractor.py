import torch
import torch.nn as nn
from pathlib import Path
from train import ExpressionMLP, CLASSES

def fold_batchnorm_into_linear(linear: nn.Linear, bn: nn.BatchNorm1d) -> tuple[torch.Tensor, torch.Tensor]:
    """
    BatchNorm을 Linear layer에 fold합니다.
    
    BatchNorm: y = gamma * (x - mean) / sqrt(var + eps) + beta
    Linear: y = W * x + b
    
    Folded: y' = W' * x + b'
    where:
        W' = gamma * W / sqrt(var + eps)
        b' = gamma * (b - mean) / sqrt(var + eps) + beta
    """
    # BatchNorm 파라미터
    gamma = bn.weight.data  # scale
    beta = bn.bias.data  # shift
    mean = bn.running_mean
    var = bn.running_var
    eps = bn.eps
    
    # Linear 파라미터
    W = linear.weight.data  # shape: (out_features, in_features)
    b = linear.bias.data if linear.bias is not None else torch.zeros(linear.out_features)
    
    # Fold 계산
    std = torch.sqrt(var + eps)
    W_folded = W * (gamma / std).unsqueeze(1)
    b_folded = gamma * (b - mean) / std + beta
    
    return W_folded, b_folded


def extract_model_to_cpp_header(model_path: str = "best_model.pth", output_path: str = "model_weights.h") -> None:
    """
    PyTorch 모델을 로드하고 BatchNorm을 fold한 뒤 C++ 헤더 파일로 저장합니다.
    """
    print("=" * 70)
    print("Model Weight Extractor (with BatchNorm Folding)")
    print("=" * 70)
    
    # 모델 로드
    print(f"\n모델 로드 중: {model_path}")
    checkpoint = torch.load(model_path, map_location="cpu")
    input_dim = int(checkpoint.get("input_dim", 0))
    
    if input_dim <= 0:
        raise ValueError("input_dim이 checkpoint에 없습니다.")
    
    model = ExpressionMLP(input_dim=input_dim, num_classes=len(CLASSES))
    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()
    
    print(f"입력 차원: {input_dim}")
    print(f"클래스 수: {len(CLASSES)}")
    print(f"클래스: {CLASSES}")
    
    # BatchNorm fold
    print("\nBatchNorm folding 중...")
    
    # Layer 1: Linear(input_dim, 1024) + BatchNorm1d(1024)
    linear1 = model.fc_stack[0]
    bn1 = model.fc_stack[1]
    W1_folded, b1_folded = fold_batchnorm_into_linear(linear1, bn1)
    print(f"  Layer 1: {W1_folded.shape} weight, {b1_folded.shape} bias")
    
    # Layer 2: Linear(1024, 512) + BatchNorm1d(512)
    linear2 = model.fc_stack[4]
    bn2 = model.fc_stack[5]
    W2_folded, b2_folded = fold_batchnorm_into_linear(linear2, bn2)
    print(f"  Layer 2: {W2_folded.shape} weight, {b2_folded.shape} bias")
    
    # Layer 3: Linear(512, num_classes) - BatchNorm 없음
    linear3 = model.fc_stack[8]
    W3 = linear3.weight.data
    b3 = linear3.bias.data
    print(f"  Layer 3: {W3.shape} weight, {b3.shape} bias")
    
    # C++ 헤더 파일 생성
    print(f"\nC++ 헤더 파일 생성 중: {output_path}")
    
    with open(output_path, 'w') as f:
        f.write("// Auto-generated model weights with folded BatchNorm\n")
        f.write("// Generated by model_extractor.py\n")
        f.write("// SIMD-friendly with proper alignment for AVX/AVX2/AVX-512\n\n")
        f.write("#ifndef MODEL_WEIGHTS_H\n")
        f.write("#define MODEL_WEIGHTS_H\n\n")
        
        # 상수 정의
        f.write(f"constexpr int INPUT_DIM = {input_dim};\n")
        f.write(f"constexpr int HIDDEN1_DIM = 1024;\n")
        f.write(f"constexpr int HIDDEN2_DIM = 512;\n")
        f.write(f"constexpr int OUTPUT_DIM = {len(CLASSES)};\n\n")
        
        # 클래스 이름
        f.write("constexpr const char* CLASS_NAMES[] = {\n")
        for i, class_name in enumerate(CLASSES):
            f.write(f'    "{class_name}"')
            if i < len(CLASSES) - 1:
                f.write(',')
            f.write('\n')
        f.write("};\n\n")
        
        # Layer 1 weights (2D array를 1D로 flatten)
        f.write("// Layer 1: Linear with folded BatchNorm\n")
        f.write("// Weights are stored in row-major order: [out_features][in_features]\n")
        f.write(f"alignas(64) const float LAYER1_WEIGHT[{W1_folded.shape[0]}][{W1_folded.shape[1]}] = {{\n")
        for i in range(W1_folded.shape[0]):
            f.write("    {")
            for j in range(W1_folded.shape[1]):
                f.write(f"{W1_folded[i, j].item():.8f}f")
                if j < W1_folded.shape[1] - 1:
                    f.write(", ")
            f.write("}")
            if i < W1_folded.shape[0] - 1:
                f.write(",")
            f.write("\n")
        f.write("};\n\n")
        
        # Layer 1 bias
        f.write(f"alignas(64) const float LAYER1_BIAS[{b1_folded.shape[0]}] = {{\n    ")
        for i in range(b1_folded.shape[0]):
            f.write(f"{b1_folded[i].item():.8f}f")
            if i < b1_folded.shape[0] - 1:
                f.write(", ")
            if (i + 1) % 4 == 0 and i < b1_folded.shape[0] - 1:
                f.write("\n    ")
        f.write("\n};\n\n")
        
        # Layer 2 weights
        f.write("// Layer 2: Linear with folded BatchNorm\n")
        f.write(f"alignas(64) const float LAYER2_WEIGHT[{W2_folded.shape[0]}][{W2_folded.shape[1]}] = {{\n")
        for i in range(W2_folded.shape[0]):
            f.write("    {")
            for j in range(W2_folded.shape[1]):
                f.write(f"{W2_folded[i, j].item():.8f}f")
                if j < W2_folded.shape[1] - 1:
                    f.write(", ")
            f.write("}")
            if i < W2_folded.shape[0] - 1:
                f.write(",")
            f.write("\n")
        f.write("};\n\n")
        
        # Layer 2 bias
        f.write(f"alignas(64) const float LAYER2_BIAS[{b2_folded.shape[0]}] = {{\n    ")
        for i in range(b2_folded.shape[0]):
            f.write(f"{b2_folded[i].item():.8f}f")
            if i < b2_folded.shape[0] - 1:
                f.write(", ")
            if (i + 1) % 4 == 0 and i < b2_folded.shape[0] - 1:
                f.write("\n    ")
        f.write("\n};\n\n")
        
        # Layer 3 weights
        f.write("// Layer 3: Output Linear\n")
        f.write(f"alignas(64) const float LAYER3_WEIGHT[{W3.shape[0]}][{W3.shape[1]}] = {{\n")
        for i in range(W3.shape[0]):
            f.write("    {")
            for j in range(W3.shape[1]):
                f.write(f"{W3[i, j].item():.8f}f")
                if j < W3.shape[1] - 1:
                    f.write(", ")
            f.write("}")
            if i < W3.shape[0] - 1:
                f.write(",")
            f.write("\n")
        f.write("};\n\n")
        
        # Layer 3 bias
        f.write(f"alignas(64) const float LAYER3_BIAS[{b3.shape[0]}] = {{\n    ")
        for i in range(b3.shape[0]):
            f.write(f"{b3[i].item():.8f}f")
            if i < b3.shape[0] - 1:
                f.write(", ")
        f.write("\n};\n\n")
        
        # LeakyReLU negative slope
        f.write("// LeakyReLU negative slope\n")
        f.write("constexpr float LEAKY_RELU_NEGATIVE_SLOPE = 0.01f;\n\n")
        
        # 간단한 forward 함수 예제
        f.write("// Example forward function\n")
        f.write("// The compiler can auto-vectorize these loops with proper flags (-O3 -march=native)\n")
        f.write("/*\n")
        f.write("inline float leaky_relu(float x, float negative_slope = LEAKY_RELU_NEGATIVE_SLOPE) {\n")
        f.write("    return x > 0.0f ? x : x * negative_slope;\n")
        f.write("}\n\n")
        f.write("void forward(const float* input, float* output) {\n")
        f.write("    // Layer 1 - compiler can auto-vectorize with proper alignment\n")
        f.write("    alignas(64) float hidden1[HIDDEN1_DIM];\n")
        f.write("    for (int i = 0; i < HIDDEN1_DIM; ++i) {\n")
        f.write("        float sum = LAYER1_BIAS[i];\n")
        f.write("        for (int j = 0; j < INPUT_DIM; ++j) {\n")
        f.write("            sum += LAYER1_WEIGHT[i][j] * input[j];\n")
        f.write("        }\n")
        f.write("        hidden1[i] = leaky_relu(sum);\n")
        f.write("    }\n\n")
        f.write("    // Layer 2\n")
        f.write("    alignas(64) float hidden2[HIDDEN2_DIM];\n")
        f.write("    for (int i = 0; i < HIDDEN2_DIM; ++i) {\n")
        f.write("        float sum = LAYER2_BIAS[i];\n")
        f.write("        for (int j = 0; j < HIDDEN1_DIM; ++j) {\n")
        f.write("            sum += LAYER2_WEIGHT[i][j] * hidden1[j];\n")
        f.write("        }\n")
        f.write("        hidden2[i] = leaky_relu(sum);\n")
        f.write("    }\n\n")
        f.write("    // Layer 3 (output)\n")
        f.write("    for (int i = 0; i < OUTPUT_DIM; ++i) {\n")
        f.write("        float sum = LAYER3_BIAS[i];\n")
        f.write("        for (int j = 0; j < HIDDEN2_DIM; ++j) {\n")
        f.write("            sum += LAYER3_WEIGHT[i][j] * hidden2[j];\n")
        f.write("        }\n")
        f.write("        output[i] = sum;\n")
        f.write("    }\n")
        f.write("}\n")
        f.write("*/\n\n")
        
        f.write("#endif // MODEL_WEIGHTS_H\n")
    
    print(f"완료! {output_path} 파일이 생성되었습니다.")
    print("\n참고:")
    print("  - BatchNorm이 Linear layer에 fold되었습니다.")
    print("  - Dropout은 inference 시 사용되지 않으므로 제외되었습니다.")
    print("  - LeakyReLU activation을 직접 구현해야 합니다.")
    print("=" * 70)

def extract_model_to_js(model_path: str = "best_model.pth", output_path: str = "../js/model_weights.js") -> None:
    """
    PyTorch 모델을 로드하고 BatchNorm을 fold한 뒤 JavaScript 파일로 저장합니다.
    """
    print("=" * 70)
    print("Model Weight Extractor to JavaScript (with BatchNorm Folding)")
    print("=" * 70)
    
    # 모델 로드
    print(f"\n모델 로드 중: {model_path}")
    checkpoint = torch.load(model_path, map_location="cpu")
    input_dim = int(checkpoint.get("input_dim", 0))
    
    if input_dim <= 0:
        raise ValueError("input_dim이 checkpoint에 없습니다.")
    
    model = ExpressionMLP(input_dim=input_dim, num_classes=len(CLASSES))
    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()
    
    print(f"입력 차원: {input_dim}")
    print(f"클래스 수: {len(CLASSES)}")
    print(f"클래스: {CLASSES}")
    
    # BatchNorm fold
    print("\nBatchNorm folding 중...")
    
    # Layer 1: Linear(input_dim, 1024) + BatchNorm1d(1024)
    linear1 = model.fc_stack[0]
    bn1 = model.fc_stack[1]
    W1_folded, b1_folded = fold_batchnorm_into_linear(linear1, bn1)
    print(f"  Layer 1: {W1_folded.shape} weight, {b1_folded.shape} bias")
    
    # Layer 2: Linear(1024, 512) + BatchNorm1d(512)
    linear2 = model.fc_stack[4]
    bn2 = model.fc_stack[5]
    W2_folded, b2_folded = fold_batchnorm_into_linear(linear2, bn2)
    print(f"  Layer 2: {W2_folded.shape} weight, {b2_folded.shape} bias")
    
    # Layer 3: Linear(512, num_classes) - BatchNorm 없음
    linear3 = model.fc_stack[8]
    W3 = linear3.weight.data
    b3 = linear3.bias.data
    print(f"  Layer 3: {W3.shape} weight, {b3.shape} bias")
    
    # JavaScript 파일 생성
    print(f"\nJavaScript 파일 생성 중: {output_path}")
    
    with open(output_path, 'w') as f:
        f.write("// Auto-generated model weights with folded BatchNorm\n")
        f.write("// Generated by model_extractor.py\n\n")
        
        # Constants
        f.write(f"export const INPUT_DIM = {input_dim};\n")
        f.write(f"export const HIDDEN1_DIM = 1024;\n")
        f.write(f"export const HIDDEN2_DIM = 512;\n")
        f.write(f"export const OUTPUT_DIM = {len(CLASSES)};\n\n")
        
        # Class names
        f.write("export const CLASS_NAMES = [\n")
        for i, class_name in enumerate(CLASSES):
            f.write(f'    "{class_name}"')
            if i < len(CLASSES) - 1:
                f.write(',')
            f.write('\n')
        f.write("];\n\n")
        
        # Layer 1 weights (flattened)
        f.write("// Layer 1 weights (flattened row-major)\n")
        f.write("export const LAYER1_WEIGHT = [\n")
        for i in range(W1_folded.shape[0]):
            f.write("    ")
            for j in range(W1_folded.shape[1]):
                f.write(f"{W1_folded[i, j].item():.8f}")
                if not (i == W1_folded.shape[0] - 1 and j == W1_folded.shape[1] - 1):
                    f.write(", ")
            f.write("\n")
        f.write("];\n\n")
        
        # Layer 1 bias
        f.write("export const LAYER1_BIAS = [\n    ")
        for i in range(b1_folded.shape[0]):
            f.write(f"{b1_folded[i].item():.8f}")
            if i < b1_folded.shape[0] - 1:
                f.write(", ")
            if (i + 1) % 4 == 0 and i < b1_folded.shape[0] - 1:
                f.write("\n    ")
        f.write("\n];\n\n")
        
        # Layer 2 weights (flattened)
        f.write("// Layer 2 weights (flattened row-major)\n")
        f.write("export const LAYER2_WEIGHT = [\n")
        for i in range(W2_folded.shape[0]):
            f.write("    ")
            for j in range(W2_folded.shape[1]):
                f.write(f"{W2_folded[i, j].item():.8f}")
                if not (i == W2_folded.shape[0] - 1 and j == W2_folded.shape[1] - 1):
                    f.write(", ")
            f.write("\n")
        f.write("];\n\n")
        
        # Layer 2 bias
        f.write("export const LAYER2_BIAS = [\n    ")
        for i in range(b2_folded.shape[0]):
            f.write(f"{b2_folded[i].item():.8f}")
            if i < b2_folded.shape[0] - 1:
                f.write(", ")
            if (i + 1) % 4 == 0 and i < b2_folded.shape[0] - 1:
                f.write("\n    ")
        f.write("\n];\n\n")
        
        # Layer 3 weights (flattened)
        f.write("// Layer 3 weights (flattened row-major)\n")
        f.write("export const LAYER3_WEIGHT = [\n")
        for i in range(W3.shape[0]):
            f.write("    ")
            for j in range(W3.shape[1]):
                f.write(f"{W3[i, j].item():.8f}")
                if not (i == W3.shape[0] - 1 and j == W3.shape[1] - 1):
                    f.write(", ")
            f.write("\n")
        f.write("];\n\n")
        
        # Layer 3 bias
        f.write("export const LAYER3_BIAS = [\n    ")
        for i in range(b3.shape[0]):
            f.write(f"{b3[i].item():.8f}")
            if i < b3.shape[0] - 1:
                f.write(", ")
        f.write("\n];\n")
    
    print(f"완료! {output_path} 파일이 생성되었습니다.")
    print("=" * 70)

if __name__ == "__main__":
    extract_model_to_cpp_header()
    extract_model_to_js()
